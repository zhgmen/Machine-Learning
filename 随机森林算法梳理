1，集成学习的概念

相对于使用一个单一的精密的高效能的学习器对数据进行处理，我们采用多个weak learner进行学习，并且通过一定的手段将这些weak learner的结果进行整合，得到最终需要的结果。

就像我们不愿意将鸡蛋放在同一个篮子里，选择多个不同的模型，如果每个的性能都有一定的保障的话，即使不必每个都太好，我们仍然能利用他们的差异对冲风险，并且得到一个优化的整体解决方案。

2，个体学习器概念

个体学习器相对于集成学习而言，表示集成学习中的一种学习器，根据类型可分为同质和异质，分别可称为基学习器和个体学习器。

（1）个体学习器间存在强依赖关系，必须串行生成的序列化方法，例如基于Booosting；

（2）个体学习器间不存在强依赖关系，可同时生成的并行化方法，例如基于Bagging和随机森林。

3，boosting与bagging

bagging：自助采样法，给定包含m个样本的数据集，随机取出一个样本放入采样集中，再把该样本放回初始数据集，这样经过m次随机采样操作，可以得到含m个样本的采样集。照这样，可以采样出T个含m个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合。结合时对多分类任务使用简单投票法，对回归任务使用简单平均法

对训练集采用有放回采样。通过对原数据集进行有放回的采样，构建出大小和原数据集T一样的新数据集T1，T2，T3…，然后用这些新的数据集训练多个分类器f1，f2，f3…。最终分类结果根据这些分类器各自结果的投票来决定。

bagging算法中，基分类器之间不存在依赖关系，基分类器可以并行生成。

bagging的性能依赖于基分类器的稳定性，如果基分类器是不稳定的，bagging有助于减低训练数据的随机扰动导致的误差，但是如果基分类器是稳定的，即对数据变化不敏感，那么bagging方法就得不到性能的提升，甚至会减低。

Boosting：先从初始训练集上训练一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器，如此反复进行，直至基学习器数目达到实现指定的值T，最终将这T个基学习器进行加权结合。Adaboost算法

是一个迭代的过程，每次在新分类器中强调上一个分类器中被错误分类的样本（增加错误分类样本的权重），最后将这些模型组合起来的方法。每次对正确分类的样本降权，对错误分类的样本加权，最后分类器是多个弱分类器的加权组合。Boosting没有对样本进行重采样，而是对样本的分布进行了调整。

boosting算法中，基分类器之间存在强依赖关系，基分类器需要串行生成。

4，组合策略

为什么要结合学习器：（1）统计方面，由于假设空间往往很大，可能有多个假设在训练集上达到同等性能。此时若使用单学习器可能因误选导致泛化性能不佳；（2）计算方面，单个学习算法容易陷入局部极小；（3）表示方面，某些学习任务的真实假设可能不在当前学习算法的假设空间，此时使用多个学习器扩大假设空间。

算法：平均法（加权（个体学习器性能相差较大），简单（性能相近）），投票法（绝对多数（超过半数标记。否则拒绝预测），相对多数，加权投票），学习法（通过另一个学习器来进行结合，Stacking算法）

Stacking算法：先从初始数据集训练出初级学习器，然后生成一个新数据集用于训练次级学习器。在这个新数据集中，初级学习器的输出被当作样例输入特征，而出事样本的标记仍被当作样例标记。

注意点：若直接用初级学习器的训练集来产生次级训练集，则过拟合风险会比较大；一般会通过交叉验证等方式，用训练初级学习器未使用的样本来产生次级学习器的训练样本。

5，随机森林的思想

是集成学习bagging的一种代表模型，随机森林模型正如他表面意思，是由若干颗树随机组成一片森林，这里的树就是决策树。

随机森林的出现主要是为了解单一决策树可能出现的很大误差和overfitting的问题。这个算法的核心思想就是将多个不同的决策树进行组合，利用这种组合降低单一决策树有可能带来的片面性和判断不准确性。用我们常说的话来形容这个思想就是“三个臭皮匠赛过诸葛亮”。

具体来讲，随机森林是用随机的方式建立一个森林，这个随机性表述的含义我们接下来会讲。随机森林是由很多的决策树组成，但每一棵决策树之间是没有关联的。在得到森林之后，当对一个新的样本进行判断或预测的时候，让森林中的每一棵决策树分别进行判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。

首先，从原始的数据集中采取有放回的抽样，构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。第二，利用子数据集来构建子决策树，将这个数据放到每个子决策树中，每个子决策树输出一个结果。最后，如果有了新的数据需要通过随机森林得到分类结果，就可以通过对子决策树的判断结果的投票，得到随机森林的输出结果了。

与数据集的随机选取类似，随机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选取最优的特征。这样能够使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。

6，随机森林的推广

随机森林的推广(Extra Trees)是RF的一个变种, 原理几乎和RF一模一样，仅有区别有：

（1） 对于每个决策树的训练集，RF采用的是随机采样bootstrap来选择采样集作为每个决策树的训练集，而extra trees一般不采用随机采样，即每个决策树采用原始训练集。

（2） 在选定了划分特征后，RF的决策树会基于信息增益，基尼系数，均方差之类的原则，选择一个最优的特征值划分点，这和传统的决策树相同。但是extra trees比较的激进，他会随机的选择一个特征值来划分决策树。

从第二点可以看出，由于随机选择了特征值的划分点位，而不是最优点位，这样会导致生成的决策树的规模一般会大于RF所生成的决策树。也就是说，模型的方差相对于RF进一步减少，但是bias相对于RF进一步增大。在某些时候，extra trees的泛化能力比RF更好

7，优缺点

优点：

a）随机森林算法能解决分类与回归两种类型的问题，表现良好，由于是集成学习，方差和偏差都比较低，泛化性能优越；

b）随机森林对于高维数据集的处理能力很好，它可以处理成千上万的输入变量，并确定最重要的变量，因此被认为是一个不错的降维方法。此外，该模型能够输出特征的重要性程度，这是一个非常实用的功能。

c) 可以应对缺失数据；

d）当存在分类不平衡的情况时，随机森林能够提供平衡数据集误差的有效方法；

e ) 高度并行化，易于分布式实现

f) 由于是树模型 ，不需要归一化即可之间使用

缺点：

a）随机森林在解决回归问题时并没有像它在分类中表现的那么好，这是因为它并不能给出一个连续型的输出。当进行回归时，随机森林不能够作出超越训练集数据范围的预测，这可能导致在对某些还有特定噪声的数据进行建模时出现过度拟合。

b）对于许多统计建模者来说，随机森林给人的感觉像是一个黑盒子——你几乎无法控制模型内部的运行，只能在不同的参数和随机种子之间进行尝试。

c) 忽略属性之间的相关性

8，sklearn参数

class sklearn.ensemble.RandomForestClassifier（
    n_estimators=10, criterion='gini', max_depth=None, 
    min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0,
    max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, 
    min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, 
    random_state=None, verbose=0, warm_start=False, class_weight=None)
sklearn中决策树的参数：

1，criterion: ”gini” or “entropy”(default=”gini”)是计算属性的gini(基尼不纯度)还是entropy(信息增益)，来选择最合适的节点。

2，splitter: ”best” or “random”(default=”best”)随机选择属性还是选择不纯度最大的属性，建议用默认。

3，max_features: 选择最适属性时划分的特征不能超过此值。

当为整数时，即最大特征数；当为小数时，训练集特征数*小数；
if “auto”, then max_features=sqrt(n_features).
If “sqrt”, thenmax_features=sqrt(n_features).
If “log2”, thenmax_features=log2(n_features).
If None, then max_features=n_features.

4，max_depth: (default=None)设置树的最大深度，默认为None，这样建树时，会使每一个叶节点只有一个类别，或是达到min_samples_split。

5，min_samples_split:根据属性划分节点时，每个划分最少的样本数。

6，min_samples_leaf:叶子节点最少的样本数。

7，max_leaf_nodes: (default=None)叶子树的最大样本数。
8，min_weight_fraction_leaf: (default=0) 叶子节点所需要的最小权值
9，verbose:(default=0) 是否显示任务进程

随机森林特有的参数：

1，n_estimators=10：决策树的个数，越多越好，但是性能就会越差，至少100左右（具体数字忘记从哪里来的了）可以达到可接受的性能和误差率。

2，bootstrap=True：是否有放回的采样。

3，oob_score=False：oob（out of band，带外）数据，即：在某次决策树训练中没有被bootstrap选中的数据。多单个模型的参数训练，我们知道可以用cross validation（cv）来进行，但是特别消耗时间，而且对于随机森林这种情况也没有大的必要，所以就用这个数据对决策树模型进行验证，算是一个简单的交叉验证。性能消耗小，但是效果不错。

4，n_jobs=1：并行job个数。这个在ensemble算法中非常重要，尤其是bagging（而非boosting，因为boosting的每次迭代之间有影响，所以很难进行并行化），因为可以并行从而提高性能。1=不并行；n：n个并行；-1：CPU有多少core，就启动多少job。

5，warm_start=False：热启动，决定是否使用上次调用该类的结果然后增加新的。

6，class_weight=None：各个label的权重。

9，应用场景

数据维度相对低（几十维），同时对准确性有较高要求时。

因为不需要很多参数调整就可以达到不错的效果，基本上不知道用什么方法的时候都可以先试一下随机森林。
